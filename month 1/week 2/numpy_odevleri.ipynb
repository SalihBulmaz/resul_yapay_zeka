{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# NumPy İleri Seviye Ödevleri\n\nBu ödevler, NumPy'ın tüm özelliklerini test etmek ve derinlemesine anlamak için tasarlanmıştır. Her ödev, farklı NumPy kavramlarını birleştirerek gerçek dünya senaryolarını simüle eder.\n\n---\n\n## Ödev 1: Finansal Portföy Analiz Sistemi\n\n### Açıklama\nBir yatırım şirketi için portföy analiz sistemi geliştirin. Sistem, farklı hisse senetlerinin performansını analiz edecek, risk hesaplamaları yapacak ve portföy optimizasyonu önerileri sunacak.\n\n### Gereksinimler\n- 10 farklı hisse senedi için 2 yıllık günlük fiyat verisi oluşturun\n- Her hisse için günlük getiri hesaplayın\n- Portföy risk analizi yapın (VaR, Sharpe ratio, beta)\n- Korelasyon matrisi oluşturun\n- Portföy optimizasyonu için weight hesaplamaları yapın\n\n### Template Kod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nfrom numpy.linalg import inv, eig\nimport matplotlib.pyplot as plt\n\n# Seed ayarlama (tekrarlanabilir sonuçlar için)\nnp.random.seed(42)\n\n# ===== VERİ OLUŞTURMA =====\n# 2 yıl = 504 iş günü\ndays = 504\nn_stocks = 10\nstock_names = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX', 'AMD', 'INTC']\n\n# Başlangıç fiyatları (gerçekçi değerler)\ninitial_prices = np.array([150, 2800, 300, 3300, 800, 350, 600, 500, 100, 50])\n\n# Her hisse için farklı volatilite ve drift parametreleri\nvolatilities = np.array([0.02, 0.025, 0.018, 0.03, 0.04, 0.035, 0.045, 0.04, 0.05, 0.025])\ndrifts = np.array([0.0005, 0.0008, 0.0006, 0.001, 0.0015, 0.0012, 0.002, 0.0018, 0.0025, 0.0004])\n\n# Fiyat matrisi oluşturma (stocks x days)\nprice_matrix = np.zeros((n_stocks, days + 1))\nprice_matrix[:, 0] = initial_prices\n\n# Random walk ile fiyat simülasyonu\nfor day in range(1, days + 1):\n    # Her hisse için günlük getiri\n    daily_returns = np.random.normal(drifts, volatilities)\n    price_matrix[:, day] = price_matrix[:, day-1] * (1 + daily_returns)\n\nprint(f\"Fiyat matrisi boyutu: {price_matrix.shape}\")\nprint(f\"İlk 5 gün için AAPL fiyatları: {price_matrix[0, :5]}\")\n\n# ===== BURADA KODUNUZ BAŞLIYOR =====\n\n# 1. Günlük getiri matrisi oluşturun (price_matrix kullanarak)\n# Hint: np.diff() ve broadcasting kullanın\n\n# 2. Her hisse için yıllık getiri, volatilite ve Sharpe ratio hesaplayın\n# Hint: 252 iş günü kullanın, risk-free rate = 0.02\n\n# 3. Korelasyon matrisi oluşturun ve en yüksek/ en düşük korelasyonlu hisse çiftlerini bulun\n\n# 4. Portföy VaR (Value at Risk) hesaplayın (95% güven seviyesi)\n# Hint: Normal dağılım varsayımı ile\n\n# 5. Minimum variance portföyü hesaplayın\n# Hint: Quadratic programming, covariance matrix kullanın\n\n# 6. Beta hesaplamaları yapın (market olarak tüm hisselerin ortalamasını kullanın)\n\n# 7. Efficient frontier analizi için farklı risk seviyelerinde portföy ağırlıkları hesaplayın\n\n# 8. Sonuçları raporlayın ve en iyi portföy kombinasyonunu önerin"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Beklenen Çıktılar\n- Günlük getiri matrisi\n- Her hisse için risk-getiri analizi\n- Korelasyon matrisi ve analizi\n- VaR hesaplamaları\n- Optimal portföy ağırlıkları\n- Efficient frontier grafiği\n\n---\n\n## Ödev 2: Görüntü İşleme ve Filtreleme Sistemi\n\n### Açıklama\nBir görüntü işleme sistemi geliştirin. Sistem, farklı filtreler uygulayacak, görüntü analizi yapacak ve görüntü kalitesini iyileştirecek.\n\n### Gereksinimler\n- Farklı boyutlarda test görüntüleri oluşturun\n- Çeşitli filtreler uygulayın (blur, sharpen, edge detection)\n- Histogram analizi yapın\n- Görüntü kalitesi metrikleri hesaplayın\n- Morphological işlemler uygulayın\n\n### Template Kod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\n\n# Seed ayarlama\nnp.random.seed(42)\n\n# ===== TEST GÖRÜNTÜLERİ OLUŞTURMA =====\ndef create_test_images():\n    \"\"\"Farklı test görüntüleri oluşturur\"\"\"\n    images = {}\n    \n    # 1. Gradient görüntü\n    height, width = 256, 256\n    x = np.linspace(0, 1, width)\n    y = np.linspace(0, 1, height)\n    X, Y = np.meshgrid(x, y)\n    images['gradient'] = (X + Y) * 255\n    \n    # 2. Noise'lu görüntü\n    images['noisy'] = np.random.normal(128, 30, (height, width))\n    images['noisy'] = np.clip(images['noisy'], 0, 255)\n    \n    # 3. Geometric shapes\n    images['shapes'] = np.zeros((height, width))\n    # Circle\n    center_y, center_x = height//2, width//2\n    Y_coords, X_coords = np.ogrid[:height, :width]\n    mask = (X_coords - center_x)**2 + (Y_coords - center_y)**2 <= 50**2\n    images['shapes'][mask] = 255\n    \n    # Rectangle\n    images['shapes'][100:150, 50:200] = 128\n    \n    # 4. Checkerboard pattern\n    checker = np.indices((height, width))\n    images['checkerboard'] = ((checker[0] // 32) + (checker[1] // 32)) % 2 * 255\n    \n    return images\n\ntest_images = create_test_images()\nprint(f\"Oluşturulan görüntü türleri: {list(test_images.keys())}\")\n\n# ===== BURADA KODUNUZ BAŞLIYOR =====\n\n# 1. Her görüntü için histogram analizi yapın\n# Hint: np.histogram() kullanın, 256 bin ile\n\n# 2. Gaussian blur filtresi uygulayın (sigma=2)\n# Hint: 2D Gaussian kernel oluşturun ve np.convolve() kullanın\n\n# 3. Sharpening filtresi uygulayın\n# Hint: Laplacian kernel kullanın\n\n# 4. Edge detection uygulayın (Sobel operator)\n# Hint: X ve Y yönünde gradient hesaplayın\n\n# 5. Median filter uygulayın (salt & pepper noise için)\n# Hint: Sliding window ile median hesaplayın\n\n# 6. Histogram equalization uygulayın\n# Hint: Cumulative distribution function kullanın\n\n# 7. Morphological işlemler uygulayın (erosion, dilation)\n# Hint: Binary görüntüler için\n\n# 8. Görüntü kalitesi metrikleri hesaplayın (PSNR, MSE)\n# Hint: Orijinal ve işlenmiş görüntüleri karşılaştırın\n\n# 9. Adaptive thresholding uygulayın\n# Hint: Local mean kullanarak\n\n# 10. Sonuçları görselleştirin ve analiz edin"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Beklenen Çıktılar\n- Her görüntü için histogram grafikleri\n- Filtrelenmiş görüntüler\n- Edge detection sonuçları\n- Kalite metrikleri tablosu\n- Morphological işlem sonuçları\n- Karşılaştırmalı analiz raporu\n\n---\n\n## Ödev 3: Makine Öğrenmesi Veri Hazırlama ve Feature Engineering\n\n### Açıklama\nKapsamlı bir makine öğrenmesi veri hazırlama sistemi geliştirin. Sistem, veri temizleme, feature engineering, normalizasyon ve veri analizi yapacak.\n\n### Gereksinimler\n- Çok boyutlu veri seti oluşturun\n- Eksik veri imputation yapın\n- Feature scaling ve normalization uygulayın\n- Outlier detection ve handling yapın\n- Feature selection ve dimensionality reduction uygulayın\n\n### Template Kod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\n\n# Seed ayarlama\nnp.random.seed(42)\n\n# ===== KOMPLEKS VERİ SETİ OLUŞTURMA =====\nn_samples = 1000\nn_features = 20\n\n# Feature isimleri\nfeature_names = [f'feature_{i}' for i in range(n_features)]\n\n# Gerçekçi veri oluşturma\ndata = {}\n\n# 1. Normal dağılımlı özellikler\ndata['feature_0'] = np.random.normal(100, 15, n_samples)  # Yaş\ndata['feature_1'] = np.random.normal(70, 12, n_samples)   # Kilo\ndata['feature_2'] = np.random.normal(170, 10, n_samples)  # Boy\n\n# 2. Kategorik özellikler (one-hot encoding için)\ndata['feature_3'] = np.random.choice(['A', 'B', 'C'], n_samples, p=[0.4, 0.3, 0.3])\ndata['feature_4'] = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n\n# 3. Correlated features\ndata['feature_5'] = data['feature_1'] * 0.8 + np.random.normal(0, 5, n_samples)  # BMI benzeri\ndata['feature_6'] = data['feature_2'] * 0.6 + np.random.normal(0, 8, n_samples)  # Boy ile ilişkili\n\n# 4. Polynomial features\ndata['feature_7'] = data['feature_0']**2 / 1000 + np.random.normal(0, 10, n_samples)\ndata['feature_8'] = np.sqrt(data['feature_1']) + np.random.normal(0, 2, n_samples)\n\n# 5. Cyclical features\ndata['feature_9'] = np.sin(2 * np.pi * np.arange(n_samples) / 365) + np.random.normal(0, 0.1, n_samples)\ndata['feature_10'] = np.cos(2 * np.pi * np.arange(n_samples) / 7) + np.random.normal(0, 0.1, n_samples)\n\n# 6. Noise features\nfor i in range(11, 20):\n    data[f'feature_{i}'] = np.random.normal(0, 1, n_samples)\n\n# DataFrame oluşturma\ndf = pd.DataFrame(data)\nprint(f\"Veri seti boyutu: {df.shape}\")\nprint(f\"Veri tipleri:\\n{df.dtypes.value_counts()}\")\n\n# Eksik veri ekleme\nmissing_mask = np.random.random(df.shape) < 0.05  # %5 eksik veri\ndf_missing = df.copy()\ndf_missing[missing_mask] = np.nan\n\nprint(f\"Eksik veri sayısı: {df_missing.isnull().sum().sum()}\")\n\n# ===== BURADA KODUNUZ BAŞLIYOR =====\n\n# 1. Eksik veri analizi ve imputation\n# Hint: Mean, median, mode ve forward fill yöntemlerini karşılaştırın\n\n# 2. Outlier detection (IQR, Z-score, Isolation Forest benzeri)\n# Hint: 3-sigma kuralı ve IQR yöntemlerini uygulayın\n\n# 3. Feature correlation analizi\n# Hint: Correlation matrix oluşturun ve yüksek korelasyonlu feature'ları tespit edin\n\n# 4. Feature scaling (StandardScaler, MinMaxScaler, RobustScaler)\n# Hint: Farklı scaling yöntemlerini karşılaştırın\n\n# 5. Polynomial feature generation\n# Hint: 2. derece polynomial features oluşturun\n\n# 6. Feature selection (Variance threshold, correlation-based)\n# Hint: Düşük varyanslı ve yüksek korelasyonlu feature'ları çıkarın\n\n# 7. Dimensionality reduction (PCA)\n# Hint: Explained variance ratio hesaplayın\n\n# 8. Categorical encoding (One-hot, Label encoding)\n# Hint: Kategorik değişkenleri sayısal hale getirin\n\n# 9. Feature importance hesaplama\n# Hint: Variance, correlation ve mutual information kullanın\n\n# 10. Data quality metrics hesaplama\n# Hint: Completeness, consistency, validity"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Beklenen Çıktılar\n- Eksik veri analizi raporu\n- Outlier detection sonuçları\n- Correlation matrix ve heatmap\n- Feature importance sıralaması\n- PCA sonuçları ve explained variance\n- Data quality metrics tablosu\n- Preprocessing pipeline önerisi\n\n---\n\n## Ödev 4: İstatistiksel Analiz ve Hipotez Testi Sistemi\n\n### Açıklama\nKapsamlı bir istatistiksel analiz sistemi geliştirin. Sistem, descriptive statistics, inferential statistics, hypothesis testing ve regression analysis yapacak.\n\n### Gereksinimler\n- Çoklu veri setleri oluşturun\n- Descriptive statistics hesaplayın\n- Parametrik ve non-parametrik testler uygulayın\n- Regression analysis yapın\n- Confidence intervals hesaplayın\n\n### Template Kod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Seed ayarlama\nnp.random.seed(42)\n\n# ===== ÇOKLU VERİ SETLERİ OLUŞTURMA =====\nn_samples = 500\n\n# 1. Normal dağılımlı veri setleri\ngroup_a = np.random.normal(100, 15, n_samples)\ngroup_b = np.random.normal(105, 18, n_samples)\ngroup_c = np.random.normal(95, 12, n_samples)\n\n# 2. Exponential dağılımlı veri\nexp_data = np.random.exponential(2, n_samples)\n\n# 3. Mixed distribution data\nmixed_data = np.concatenate([\n    np.random.normal(50, 5, n_samples//2),\n    np.random.normal(80, 8, n_samples//2)\n])\n\n# 4. Time series data\ntime_series = np.cumsum(np.random.normal(0, 1, n_samples)) + 100\n\n# 5. Paired data (before/after)\nbefore = np.random.normal(75, 10, n_samples)\nafter = before + np.random.normal(5, 3, n_samples)  # Treatment effect\n\n# 6. Categorical data\ncategories = np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.3, 0.5, 0.2])\ncategory_values = np.random.normal(100, 20, n_samples)\n\n# 7. Regression data\nx_reg = np.random.uniform(0, 100, n_samples)\ny_reg = 2 * x_reg + 10 + np.random.normal(0, 15, n_samples)\n\nprint(f\"Veri setleri oluşturuldu:\")\nprint(f\"Group A: mean={np.mean(group_a):.2f}, std={np.std(group_a):.2f}\")\nprint(f\"Group B: mean={np.mean(group_b):.2f}, std={np.std(group_b):.2f}\")\nprint(f\"Group C: mean={np.mean(group_c):.2f}, std={np.std(group_c):.2f}\")\n\n# ===== BURADA KODUNUZ BAŞLIYOR =====\n\n# 1. Descriptive statistics (tüm gruplar için)\n# Hint: Mean, median, mode, std, variance, skewness, kurtosis\n\n# 2. Normality tests (Shapiro-Wilk, Anderson-Darling)\n# Hint: Her grup için normality testi uygulayın\n\n# 3. Homogeneity of variance tests (Levene, Bartlett)\n# Hint: Gruplar arası variance eşitliğini test edin\n\n# 4. One-way ANOVA ve post-hoc tests\n# Hint: Tukey HSD testi uygulayın\n\n# 5. T-tests (independent ve paired)\n# Hint: Group A vs Group B ve before vs after\n\n# 6. Non-parametric tests (Mann-Whitney, Kruskal-Wallis)\n# Hint: Normal dağılım varsayımı olmayan durumlar için\n\n# 7. Correlation analysis (Pearson, Spearman, Kendall)\n# Hint: Farklı correlation türlerini karşılaştırın\n\n# 8. Linear regression analysis\n# Hint: x_reg ve y_reg için regression yapın\n\n# 9. Confidence intervals hesaplama\n# Hint: Mean, proportion ve regression coefficients için\n\n# 10. Effect size hesaplama (Cohen's d, eta-squared)\n# Hint: Test sonuçlarının pratik önemini değerlendirin\n\n# 11. Power analysis\n# Hint: Sample size hesaplamaları yapın\n\n# 12. Multiple comparisons correction (Bonferroni, FDR)\n# Hint: Type I error kontrolü"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Beklenen Çıktılar\n- Descriptive statistics tablosu\n- Normality test sonuçları\n- ANOVA ve post-hoc test sonuçları\n- T-test sonuçları ve effect sizes\n- Correlation matrix ve p-values\n- Regression analysis sonuçları\n- Confidence intervals tablosu\n- Statistical power analizi\n- Multiple comparisons correction sonuçları\n\n---\n\n## Ödev 5: Optimizasyon ve Simülasyon Sistemi\n\n### Açıklama\nKapsamlı bir optimizasyon ve simülasyon sistemi geliştirin. Sistem, Monte Carlo simülasyonları, optimizasyon algoritmaları ve sensitivity analysis yapacak.\n\n### Gereksinimler\n- Monte Carlo simülasyonları yapın\n- Gradient descent optimizasyonu uygulayın\n- Sensitivity analysis yapın\n- Constraint optimization çözün\n- Stochastic processes simüle edin\n\n### Template Kod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm, uniform\n\n# Seed ayarlama\nnp.random.seed(42)\n\n# ===== OPTIMIZASYON PROBLEMLERİ TANIMLAMA =====\n\n# 1. Rosenbrock function (test function)\ndef rosenbrock(x):\n    \"\"\"Rosenbrock function: f(x,y) = (1-x)^2 + 100(y-x^2)^2\"\"\"\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\n# 2. Himmelblau function\ndef himmelblau(x):\n    \"\"\"Himmelblau function: f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\"\"\"\n    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n\n# 3. Portfolio optimization function\ndef portfolio_risk(weights, returns):\n    \"\"\"Portfolio risk (variance) hesaplama\"\"\"\n    cov_matrix = np.cov(returns.T)\n    return weights.T @ cov_matrix @ weights\n\ndef portfolio_return(weights, returns):\n    \"\"\"Portfolio return hesaplama\"\"\"\n    return np.sum(weights * np.mean(returns, axis=0))\n\n# 4. Manufacturing cost function\ndef manufacturing_cost(params):\n    \"\"\"Manufacturing cost function: C = a*x^2 + b*y + c*z^3\"\"\"\n    x, y, z = params\n    a, b, c = 2, 3, 1\n    return a*x**2 + b*y + c*z**3\n\n# Test data oluşturma\nn_assets = 5\nn_days = 252\nreturns_data = np.random.normal(0.001, 0.02, (n_days, n_assets))\n\nprint(f\"Optimizasyon problemleri tanımlandı\")\nprint(f\"Portfolio data shape: {returns_data.shape}\")\n\n# ===== BURADA KODUNUZ BAŞLIYOR =====\n\n# 1. Gradient descent implementasyonu\n# Hint: Rosenbrock function için gradient descent yazın\n\n# 2. Monte Carlo integration\n# Hint: Pi sayısını Monte Carlo ile hesaplayın\n\n# 3. Portfolio optimization (Markowitz)\n# Hint: Risk-return trade-off optimizasyonu\n\n# 4. Constraint optimization\n# Hint: Manufacturing cost minimization with constraints\n\n# 5. Global optimization (random search, simulated annealing)\n# Hint: Himmelblau function için global minimum bulun\n\n# 6. Sensitivity analysis\n# Hint: Parameter sensitivity hesaplayın\n\n# 7. Stochastic processes simulation\n# Hint: Random walk, Brownian motion simüle edin\n\n# 8. Bootstrap sampling ve confidence intervals\n# Hint: Portfolio returns için bootstrap CI hesaplayın\n\n# 9. Multi-objective optimization\n# Hint: Risk ve return arasında Pareto optimal çözümler\n\n# 10. Robust optimization\n# Hint: Uncertainty altında robust çözümler\n\n# 11. Time series optimization\n# Hint: Moving average parameters optimization\n\n# 12. Neural network weight optimization (basit)\n# Hint: Simple neural network için gradient descent"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Beklenen Çıktılar\n- Gradient descent convergence grafiği\n- Monte Carlo integration sonuçları\n- Portfolio efficient frontier\n- Constraint optimization sonuçları\n- Global optimization convergence\n- Sensitivity analysis heatmap\n- Stochastic process simülasyonları\n- Bootstrap confidence intervals\n- Multi-objective optimization Pareto front\n- Robust optimization sonuçları\n- Time series optimization grafikleri\n- Neural network training curves\n\n---\n\n## Ödev Değerlendirme Kriterleri\n\n### Her Ödev İçin:\n1. **Kod Kalitesi** (20%): Temiz, okunabilir, modüler kod\n2. **Doğruluk** (30%): Matematiksel hesaplamaların doğruluğu\n3. **Kapsamlılık** (25%): Tüm gereksinimlerin karşılanması\n4. **Analiz Derinliği** (15%): Sonuçların yorumlanması\n5. **Görselleştirme** (10%): Etkili grafikler ve tablolar\n\n### Bonus Puanlar:\n- Ek özellikler ekleme\n- Performans optimizasyonu\n- Yaratıcı çözümler\n- Dokümantasyon kalitesi\n\n### Teslim Formatı:\n- Jupyter notebook (.ipynb)\n- Markdown açıklamalar\n- Kod yorumları\n- Sonuç analizleri\n- Görselleştirmeler\n\nHer ödev, NumPy'ın farklı yönlerini test eder ve gerçek dünya uygulamalarını simüle eder. Ödevler zorluk seviyesi artacak şekilde sıralanmıştır."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}